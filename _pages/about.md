---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a 4th-year Ph.D. student in the [Conversational AI Group](http://coai.cs.tsinghua.edu.cn/) at the Department of Computer Science and Technology, Tsinghua University, 
under the supervision of Prof. [Minlie Huang](http://coai.cs.tsinghua.edu.cn/hml). 
Previously, I was an intern at Microsoft Research Asia, where I was mentored by Dr. [Li Dong](https://dong.li/). 
My research interests focus on developing efficient methods for the entire life cycle of language models, including pre-training, downstream adaptation, and inference.

Education
======

+ 2021.9 - Present: Ph.D. Student, Department of Computer Science and Technology, Tsinghua University
+ 2017.9 - 2021.6: B.Eng., Department of Computer Science and Technology, Tsinghua University

Publications
======

**Conference Papers**

+ **Yuxian Gu**, Li Dong, Furu Wei, Minlie Huang. MiniLLM: Knowledge Distillation of Large Language Models. **ICLR 2024**. [[pdf]](https://arxiv.org/abs/2306.08543) [[code]](https://aka.ms/MiniLLM) [[huggingface]](https://huggingface.co/MiniLLM)

+ **Yuxian Gu**, Li Dong, Furu Wei, Minlie Huang. Pre-Training to Learn in Context. **ACL 2023** (Main, Long Paper, **Oral**). [[pdf]](https://arxiv.org/pdf/2305.09137.pdf) [[code]](https://github.com/thu-coai/PICL)

+ **Yuxian Gu**, Pei Ke, Xiaoyan Zhu, Minlie Huang. Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization. **EMNLP 2022** (Main, Long Paper, **Oral**). [[pdf]](https://arxiv.org/pdf/2210.09175.pdf) [[code]](https://github.com/thu-coai/UDIT)

+ **Yuxian Gu\***, Xu Han\*, Zhiyuan Liu, Minlie Huang. PPT: Pre-Trained Prompt Tuning for Few-Shot Learning. **ACL 2022** (Main, Long Paper). [[pdf]](https://aclanthology.org/2022.acl-long.576.pdf) [[code]](https://github.com/thu-coai/PPT)

+ Daixuan Cheng, **Yuxian Gu**, Shaohan Huang, Junyu Bi, Minlie Huang, Furu Wei. Instruction Pre-Training: Language Models are Supervised Multitask Learners. **EMNLP 2024** (Main, Long Paper). [[pdf]](https://arxiv.org/pdf/2406.14491) [[code]](https://github.com/microsoft/LMOps/tree/main/instruction_pretrain) [[huggingface]](https://huggingface.co/instruction-pretrain)

+ Qi Zhu, **Yuxian Gu**, Lingxiao Luo, Bing Li, Cheng Li, Wei Peng, Minlie Huang, Xiaoyan Zhu. When does Further Pre-Training MLM Help? An Empirical Study on Task-Oriented Dialog Pre-Training. **EMNLP 2021 Workshop** (***Best Paper***). [[pdf]](https://aclanthology.org/2021.insights-1.9.pdf) [[code]](https://github.com/zqwerty/ToDDAPT)

+ **Yuxian Gu**, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, Maosong Sun. Train No Evil: Selective Masking for Task-Guided Pre-Training. **EMNLP 2020** (Short Paper). [[pdf]](https://aclanthology.org/2020.emnlp-main.566.pdf) [[code]](https://github.com/thunlp/SelectiveMasking)

+ Xin Lv, **Yuxian Gu**, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu. Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations. **EMNLP 2019** (Short Paper). [[pdf]](https://aclanthology.org/D19-1334.pdf) [[code]](https://github.com/THU-KEG/MetaKGR)

**Journal Papers**

+ **Yuxian Gu\***, Jiaxin Wen\*, Hao Sun\*, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Lei Liu, Xiaoyan Zhu, Minlie Huang. EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. 2022. **Machine Intelligence Research** [[pdf]](https://link.springer.com/article/10.1007/s11633-022-1387-3) [[code]](https://github.com/thu-coai/EVA/)

+ Zhengyan Zhang\*, **Yuxian Gu\***, Xu Han\*, Shengqi Chen\*, Chaojun Xiao\*, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun. CPM-2: Large-Scale Cost-Effective Pre-Trained Language Models. 2022. **AI Open**. [[pdf]](https://www.sciencedirect.com/science/article/pii/S2666651021000310/pdfft?md5=46efc536c128aefd0ff69139f8627ddb&pid=1-s2.0-S2666651021000310-main.pdf) [[pre-train code]](https://github.com/TsinghuaAI/CPM-2-Pretrain) [[fine-tune code]](https://github.com/TsinghuaAI/CPM-1-Finetune)

+ Xu Han\*, Zhengyan Zhang\*, Ning Ding\*, **Yuxian Gu\***, Xiao Liu\*, Yuqi Huo\*, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, Jun Zhu. Pre-Trained Models: Past, Present and Future. 2022. **AI Open**. [[pdf]](https://www.sciencedirect.com/science/article/pii/S2666651021000231/pdfft?md5=e87250d675adde41b6836aed4df648b4&pid=1-s2.0-S2666651021000231-main.pdf)

+ Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, **Yuxian Gu**, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun. CPM: A Large-Scale Generative Chinese Pre-Trained Language Model. 2021. **AI Open**. [[pdf]](https://www.sciencedirect.com/science/article/pii/S266665102100019X/pdfft?md5=c9c82038f6f237b8708270ed0fbbf80b&pid=1-s2.0-S266665102100019X-main.pdf) [[pre-train code]](https://github.com/TsinghuaAI/CPM-1-Pretrain) [[fine-tune code]](https://github.com/TsinghuaAI/CPM-1-Finetune) [[inference code]](https://github.com/TsinghuaAI/CPM-1-Generate)

**Preprints**

+ **Yuxian Gu**, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang. MiniPLM: Knowledge Distillation for Pre-Training Language Models. arXiv preprint 2024. [[pdf]](https://arxiv.org/pdf/2410.17215.pdf) [[code]](https://github.com/thu-coai/MiniPLM) [[huggingface]](https://huggingface.co/MiniLLM)

+ **Yuxian Gu**, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei. Data Selection via Optimal Control for Language Models. arXiv preprint 2024. [[pdf]](https://arxiv.org/pdf/2410.07064.pdf) [[code]](https://github.com/microsoft/LMOps/tree/main/data_selection) [[huggingface]](https://huggingface.co/Data-Selection)

+ **Yuxian Gu**, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei. Towards Optimal Learning of Language Models. arXiv preprint 2024. [[pdf]](https://arxiv.org/pdf/2402.17759.pdf) [[code]](https://github.com/microsoft/LMOps/tree/main/learning_law)

+ Yixing Li, **Yuxian Gu**, Li Dong, Dequan Wang, Yu Cheng, Furu Wei. Direct Preference Knowledge Distillation for Large Language Models. arXiv preprint 2024. [[pdf]](https://arxiv.org/pdf/2406.19774.pdf) [[code]](https://github.com/microsoft/LMOps/tree/main/dpkd)

+ Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, **Yuxian Gu**, Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples. [[pdf]](https://arxiv.org/pdf/2212.06713.pdf) [[code]](https://github.com/microsoft/LMOps/tree/main/structured_prompting/)

Services
======
+ Program Committee Member (Conference Reviewer) EMNLP 2022-2023, ACL 2023-2024, ARR 2023-2024, NeurIPS 2024

Teaching
======
I was a TA for the following undergraduate courses:

+ Artificial Neural Network (2020 Fall, 2021 Fall, 2022 Fall, 2023 Fall)
+ Object-Oriented Programming (2021 Spring, 2022 Spring, 2023 Spring, 2024 Spring)

Selected Honors and Awards
======

+ Excellent Graduate, Tsinghua University, 2021
+ Outstanding Graduate, Dept. CST, Tsinghua University, 2021
+ Outstanding Undergraduate Dissertation, Tsinghua University, 2021
+ Overall Scholarship, Dept. CST, Tsinghua University, 2020
+ Science and Technology Innovation Excellence Scholarship, Dept. CST, Tsinghua University, 2019
+ Silver Prize, Asia Student Supercomputer Challenge, Asia Supercomputer Community, 2019
+ Overall Scholarship, Dept. CST, Tsinghua University, 2018

Miscellaneous
======

I like to play [Jianzi, Jianqiu](https://en.wikipedia.org/wiki/Jianzi), a competitive sport similar to volleyball. I am the captain of Tsinghua Jianqiu Team and was the [captain](/images/jianqiu.JPG) of the Beijing Youth Jianqiu Team. I once won the [championship](/images/champion.JPG) of the Jianqiu Competition in Beijing and represented Beijing in [National Youth Olympic Games](https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E5%AD%A6%E7%94%9F%EF%BC%88%E9%9D%92%E5%B9%B4%EF%BC%89%E8%BF%90%E5%8A%A8%E4%BC%9A).

<!-- I have also created [a set of Jupyter notebooks](https://github.com/academicpages/academicpages.github.io/tree/master/markdown_generator
) that converts a CSV containing structured data about talks or presentations into individual markdown files that will be properly formatted for the academicpages template. The sample CSVs in that directory are the ones I used to create my own personal website at stuartgeiger.com. My usual workflow is that I keep a spreadsheet of my publications and talks, then run the code in these notebooks to generate the markdown files, then commit and push them to the GitHub repository.

How to edit your site's GitHub repository
------
Many people use a git client to create files on their local computer and then push them to GitHub's servers. If you are not familiar with git, you can directly edit these configuration and markdown files directly in the github.com interface. Navigate to a file (like [this one](https://github.com/academicpages/academicpages.github.io/blob/master/_talks/2012-03-01-talk-1.md) and click the pencil icon in the top right of the content preview (to the right of the "Raw | Blame | History" buttons). You can delete a file by clicking the trashcan icon to the right of the pencil icon. You can also create new files or upload files by navigating to a directory and clicking the "Create new file" or "Upload files" buttons. 

Example: editing a markdown file for a talk
![Editing a markdown file for a talk](/images/editing-talk.png)

For more info
------
More info about configuring academicpages can be found in [the guide](https://academicpages.github.io/markdown/). The [guides for the Minimal Mistakes theme](https://mmistakes.github.io/minimal-mistakes/docs/configuration/) (which this theme was forked from) might also be helpful. -->
